# üõ°Ô∏è Moderation API

**Detecci√≥n de Contenido T√≥xico con Inteligencia Artificial**

Identifica autom√°ticamente contenido ofensivo, insultos, amenazas, discurso de odio y otros tipos de toxicidad en texto. Esencial para moderar comentarios, chats y contenido generado por usuarios.

---

## üìç Informaci√≥n General

| Propiedad | Valor |
|-----------|-------|
| **URL Base** | `https://apisdom.com/api/v1` |
| **M√©todo** | `POST` |
| **Autenticaci√≥n** | API Key (Header `X-API-Key`) |
| **Tipo de Cr√©dito** | `text` |
| **Coste por llamada** | 1 cr√©dito |
| **Modelo IA** | Toxic-BERT (fine-tuned en Jigsaw dataset) |
| **Categor√≠as detectadas** | M√∫ltiples (seg√∫n modelo toxic-bert) |

### ‚è±Ô∏è Rate Limits

| Plan | L√≠mite | Cuota Mensual | Precio |
|------|--------|---------------|--------|
| **Prueba Gratuita** | 10 req/min | 1,000 cr√©ditos (√∫nico uso) | ‚Ç¨0 |
| **Plan Starter** | 60 req/min | 10,000 cr√©ditos/mes | ‚Ç¨4.99/mes |
| **Plan Pro** | 300 req/min | 100,000 cr√©ditos/mes | ‚Ç¨19.99/mes |

> **Nota sobre cuotas:**
> - **Plan Gratuito**: Los 1,000 cr√©ditos son de uso √∫nico y NO se resetean.
> - **Planes de pago**: Las cuotas se resetean el d√≠a 1 de cada mes a las 00:00 UTC.
> - Si excedes el rate limit, recibir√°s error `429 Too Many Requests` con header `Retry-After`.

### üìä Headers Informativos

La API devuelve headers que te permiten controlar tu consumo:

| Header | Descripci√≥n |
|--------|-------------|
| `X-RateLimit-Limit` | Tu l√≠mite de peticiones por minuto |
| `X-RateLimit-Remaining` | Peticiones restantes en la ventana actual |
| `Retry-After` | Segundos a esperar si recibes 429 |

---

## üîê Autenticaci√≥n

Todas las peticiones requieren tu API Key en el header `X-API-Key`:

```
X-API-Key: tu_api_key_aqui
```

Puedes obtener tu API Key desde el panel de usuario en [apisdom.com/dashboard](https://apisdom.com/dashboard).

---

## üì• Endpoint: Moderar Contenido

```
POST https://apisdom.com/api/v1/moderacion
```

### Request Body

| Campo | Tipo | Requerido | Descripci√≥n |
|-------|------|-----------|-------------|
| `text` | string | ‚úÖ S√≠ | Texto a moderar. M√≠nimo 1 car√°cter, m√°ximo 5000. |

### Ejemplo de Request

```json
{
  "text": "Eres un completo idiota y deber√≠as desaparecer."
}
```

### Response Exitosa (200 OK)

```json
{
  "text": "Eres un completo idiota y deber√≠as desaparecer.",
  "is_toxic": true,
  "toxicity_score": 0.923,
  "categories": {
    "toxic": 0.923,
    "insult": 0.876
  },
  "warning": null,
  "info_message": null
}
```

> **Nota:** El campo `categories` contiene SOLO las etiquetas que el modelo detecta con confianza significativa. Las claves exactas dependen del modelo `unitary/toxic-bert` y pueden variar.

### Campos de la Respuesta

| Campo | Tipo | Descripci√≥n |
|-------|------|-------------|
| `text` | string | El texto que fue analizado |
| `is_toxic` | boolean | `true` si toxicity_score > 0.7, `false` en caso contrario |
| `toxicity_score` | float | Puntuaci√≥n m√°xima de toxicidad detectada (0.0 a 1.0) |
| `categories` | object | Desglose por categor√≠as detectadas por el modelo (ver nota abajo) |
| `warning` | string \| null | Aviso si el texto fue truncado (m√°s de 512 tokens) |
| `info_message` | string \| null | Mensaje informativo para usuarios del plan gratuito |

### Sobre las Categor√≠as de Toxicidad

> **Nota t√©cnica:** Las categor√≠as devueltas dependen del modelo `unitary/toxic-bert`. El modelo retorna las etiquetas detectadas con su puntuaci√≥n de confianza. Las categor√≠as m√°s comunes incluyen:

| Categor√≠a | Descripci√≥n |
|-----------|-------------|
| `toxic` | Contenido t√≥xico general |
| `severe_toxic` | Toxicidad severa/extrema |
| `obscene` | Lenguaje obsceno o vulgar |
| `threat` | Amenazas o intimidaci√≥n |
| `insult` | Insultos directos |
| `identity_hate` | Discurso de odio por identidad |

**Importante:** La estructura exacta del campo `categories` puede variar seg√∫n la versi√≥n del modelo. Siempre itera sobre las claves devueltas en lugar de asumir nombres espec√≠ficos.

---

## üíª Ejemplos de C√≥digo

### Python

```python
import requests
from typing import Optional

API_URL = "https://apisdom.com/api/v1/moderacion"
API_KEY = "tu_api_key_aqui"

def moderar_contenido(texto: str) -> dict:
    """
    Analiza un texto para detectar contenido t√≥xico.
    
    Args:
        texto: String a moderar (m√°x 5000 caracteres)
    
    Returns:
        dict con is_toxic, toxicity_score y categor√≠as
    """
    response = requests.post(
        API_URL,
        headers={
            "X-API-Key": API_KEY,
            "Content-Type": "application/json"
        },
        json={"text": texto}
    )
    
    if response.status_code == 200:
        return response.json()
    elif response.status_code == 402:
        raise Exception("Sin cr√©ditos. Recarga tu saldo en apisdom.com")
    else:
        raise Exception(f"Error: {response.status_code} - {response.text}")

def debe_bloquear(resultado: dict, umbral: float = 0.7) -> bool:
    """
    Determina si un contenido debe ser bloqueado.
    
    Args:
        resultado: Respuesta de moderar_contenido()
        umbral: Puntuaci√≥n m√≠nima para bloquear (default 0.7)
    
    Returns:
        True si debe bloquearse, False si puede publicarse
    """
    # Bloquear si toxicidad general supera umbral
    if resultado['toxicity_score'] >= umbral:
        return True
    
    # Bloquear si hay amenazas o ataques de identidad (tolerancia cero)
    categorias = resultado['categories']
    if categorias.get('threat', 0) >= 0.5:
        return True
    if categorias.get('identity_attack', 0) >= 0.5:
        return True
    
    return False

# Ejemplo de uso
texto_usuario = "Gracias por tu ayuda, eres genial!"
resultado = moderar_contenido(texto_usuario)

if debe_bloquear(resultado):
    print("‚ùå BLOQUEADO - Contenido inapropiado")
    print(f"   Raz√≥n: Toxicidad {resultado['toxicity_score']:.0%}")
else:
    print("‚úÖ APROBADO - Contenido apropiado")
    print(f"   Toxicidad: {resultado['toxicity_score']:.0%}")

# Output:
# ‚úÖ APROBADO - Contenido apropiado
#    Toxicidad: 2%
```

### JavaScript / Node.js

```javascript
const API_URL = 'https://apisdom.com/api/v1/moderacion';
const API_KEY = 'tu_api_key_aqui';

async function moderarContenido(texto) {
  /**
   * Analiza un texto para detectar contenido t√≥xico.
   * @param {string} texto - Texto a moderar (m√°x 5000 caracteres)
   * @returns {Promise<Object>} - Resultado con is_toxic, toxicity_score, categories
   */
  const response = await fetch(API_URL, {
    method: 'POST',
    headers: {
      'X-API-Key': API_KEY,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({ text: texto })
  });

  if (response.status === 402) {
    throw new Error('Sin cr√©ditos. Recarga tu saldo en apisdom.com');
  }

  if (!response.ok) {
    throw new Error(`Error: ${response.status}`);
  }

  return response.json();
}

function debeBloquear(resultado, umbral = 0.7) {
  /**
   * Determina si un contenido debe ser bloqueado.
   */
  if (resultado.toxicity_score >= umbral) return true;
  if (resultado.categories.threat >= 0.5) return true;
  if (resultado.categories.identity_attack >= 0.5) return true;
  return false;
}

// Ejemplo de uso con async/await
async function procesarComentario(comentario) {
  try {
    const resultado = await moderarContenido(comentario);
    
    if (debeBloquear(resultado)) {
      console.log('‚ùå Comentario bloqueado');
      console.log(`   Toxicidad: ${(resultado.toxicity_score * 100).toFixed(0)}%`);
      
      // Identificar la raz√≥n principal
      const categorias = resultado.categories;
      const razon = Object.entries(categorias)
        .filter(([_, score]) => score >= 0.5)
        .sort((a, b) => b[1] - a[1])
        .map(([cat, score]) => `${cat}: ${(score * 100).toFixed(0)}%`);
      
      if (razon.length > 0) {
        console.log(`   Detalle: ${razon.join(', ')}`);
      }
      
      return { aprobado: false, razon: resultado };
    } else {
      console.log('‚úÖ Comentario aprobado');
      return { aprobado: true };
    }
  } catch (error) {
    console.error('Error moderando:', error.message);
    throw error;
  }
}

// Ejemplo
procesarComentario('Este tutorial es muy √∫til, gracias por compartir!');
// Output: ‚úÖ Comentario aprobado
```

### cURL

```bash
# Ejemplo b√°sico
curl -X POST "https://apisdom.com/api/v1/moderacion" \
  -H "X-API-Key: tu_api_key_aqui" \
  -H "Content-Type: application/json" \
  -d '{"text": "Excelente art√≠culo, muy bien explicado."}'

# Con jq para formatear la respuesta
curl -s -X POST "https://apisdom.com/api/v1/moderacion" \
  -H "X-API-Key: tu_api_key_aqui" \
  -H "Content-Type: application/json" \
  -d '{"text": "Este contenido de ejemplo es totalmente inocente."}' | jq .
```

### PHP

```php
<?php
$api_url = 'https://apisdom.com/api/v1/moderacion';
$api_key = 'tu_api_key_aqui';

function moderarContenido($texto) {
    global $api_url, $token;
    
    $ch = curl_init($api_url);
    curl_setopt_array($ch, [
        CURLOPT_RETURNTRANSFER => true,
        CURLOPT_POST => true,
        CURLOPT_HTTPHEADER => [
            'X-API-Key: ' . $api_key,
            'Content-Type: application/json'
        ],
        CURLOPT_POSTFIELDS => json_encode(['text' => $texto])
    ]);
    
    $response = curl_exec($ch);
    $httpCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);
    curl_close($ch);
    
    if ($httpCode === 402) {
        throw new Exception('Sin cr√©ditos. Recarga tu saldo en apisdom.com');
    }
    
    return json_decode($response, true);
}

function debeBloquear($resultado, $umbral = 0.7) {
    if ($resultado['toxicity_score'] >= $umbral) return true;
    if ($resultado['categories']['threat'] >= 0.5) return true;
    if ($resultado['categories']['identity_attack'] >= 0.5) return true;
    return false;
}

// Ejemplo de uso en un formulario
$comentario = $_POST['comentario'] ?? 'Texto de prueba amigable';

try {
    $resultado = moderarContenido($comentario);
    
    if (debeBloquear($resultado)) {
        echo "‚ùå Tu comentario no puede ser publicado.\n";
        echo "Raz√≥n: Contenido potencialmente ofensivo detectado.\n";
    } else {
        echo "‚úÖ Comentario publicado correctamente.\n";
        // Aqu√≠ guardar√≠as el comentario en la base de datos
    }
} catch (Exception $e) {
    echo "Error: " . $e->getMessage();
}
?>
```

### C# / .NET

```csharp
using System.Net.Http;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;

public class ModerationApiClient
{
    private readonly HttpClient _client;
    private const string API_URL = "https://apisdom.com/api/v1/moderacion";

    public ModerationApiClient(string apiKey)
    {
        _client = new HttpClient();
        _client.DefaultRequestHeaders.Add("X-API-Key", apiKey);
    }

    public async Task<ModerationResult> ModerarContenidoAsync(string texto)
    {
        var content = new StringContent(
            JsonSerializer.Serialize(new { text = texto }),
            Encoding.UTF8,
            "application/json"
        );

        var response = await _client.PostAsync(API_URL, content);

        if (response.StatusCode == System.Net.HttpStatusCode.PaymentRequired)
        {
            throw new Exception("Sin cr√©ditos. Recarga tu saldo en apisdom.com");
        }

        response.EnsureSuccessStatusCode();
        
        var json = await response.Content.ReadAsStringAsync();
        var options = new JsonSerializerOptions 
        { 
            PropertyNameCaseInsensitive = true 
        };
        return JsonSerializer.Deserialize<ModerationResult>(json, options);
    }

    public bool DebeBloquear(ModerationResult resultado, double umbral = 0.7)
    {
        if (resultado.ToxicityScore >= umbral) return true;
        if (resultado.Categories.Threat >= 0.5) return true;
        if (resultado.Categories.IdentityAttack >= 0.5) return true;
        return false;
    }
}

public class ModerationResult
{
    public string Text { get; set; }
    
    [JsonPropertyName("is_toxic")]
    public bool IsToxic { get; set; }
    
    [JsonPropertyName("toxicity_score")]
    public double ToxicityScore { get; set; }
    
    public ToxicityCategories Categories { get; set; }
    
    public string? Warning { get; set; }
    
    [JsonPropertyName("info_message")]
    public string? InfoMessage { get; set; }
}

public class ToxicityCategories
{
    public double Toxicity { get; set; }
    
    [JsonPropertyName("severe_toxicity")]
    public double SevereToxicity { get; set; }
    
    public double Obscene { get; set; }
    public double Threat { get; set; }
    public double Insult { get; set; }
    
    [JsonPropertyName("identity_attack")]
    public double IdentityAttack { get; set; }
    
    [JsonPropertyName("sexual_explicit")]
    public double SexualExplicit { get; set; }
}

// Ejemplo de uso
var client = new ModerationApiClient("tu_api_key_aqui");
var resultado = await client.ModerarContenidoAsync("Gracias por la informaci√≥n!");

if (client.DebeBloquear(resultado))
{
    Console.WriteLine($"‚ùå Bloqueado - Toxicidad: {resultado.ToxicityScore:P0}");
}
else
{
    Console.WriteLine($"‚úÖ Aprobado - Toxicidad: {resultado.ToxicityScore:P0}");
}
```

---

## üìä Casos de Uso Pr√°cticos

### 1. Sistema de Moderaci√≥n de Comentarios

```python
class SistemaModeraci√≥n:
    def __init__(self, api_key):
        self.api_key = api_key
        self.api_url = "https://apisdom.com/api/v1/moderacion"
    
    def evaluar(self, texto):
        """Eval√∫a un texto y devuelve la acci√≥n recomendada."""
        resultado = self._llamar_api(texto)
        
        # Clasificar seg√∫n severidad
        if resultado['categories']['severe_toxicity'] >= 0.6:
            return {
                'accion': 'BLOQUEAR_USUARIO',
                'razon': 'Contenido severamente t√≥xico',
                'notificar_admin': True
            }
        elif resultado['categories']['threat'] >= 0.5:
            return {
                'accion': 'BLOQUEAR_Y_REVISAR',
                'razon': 'Amenaza detectada',
                'notificar_admin': True
            }
        elif resultado['toxicity_score'] >= 0.7:
            return {
                'accion': 'RECHAZAR',
                'razon': 'Contenido t√≥xico',
                'notificar_admin': False
            }
        elif resultado['toxicity_score'] >= 0.4:
            return {
                'accion': 'COLA_REVISION',
                'razon': 'Contenido dudoso - requiere revisi√≥n humana',
                'notificar_admin': False
            }
        else:
            return {
                'accion': 'APROBAR',
                'razon': None,
                'notificar_admin': False
            }
    
    def _llamar_api(self, texto):
        # ... implementaci√≥n de la llamada HTTP
        pass

# Uso
moderador = SistemaModeraci√≥n("tu_api_key_aqui")
resultado = moderador.evaluar("Tu comentario aqu√≠")
print(f"Acci√≥n: {resultado['accion']}")
```

### 2. Chat en Tiempo Real con Filtro

```javascript
class ChatModerado {
  constructor(token) {
    this.token = token;
    this.historialUsuario = new Map(); // Para tracking de usuarios
  }

  async procesarMensaje(userId, mensaje) {
    const resultado = await moderarContenido(mensaje);
    
    // Actualizar historial del usuario
    const historial = this.historialUsuario.get(userId) || { infracciones: 0 };
    
    if (resultado.is_toxic) {
      historial.infracciones++;
      this.historialUsuario.set(userId, historial);
      
      if (historial.infracciones >= 3) {
        return {
          publicar: false,
          accion: 'SILENCIAR_USUARIO',
          mensaje: 'Has sido silenciado por comportamiento inapropiado.'
        };
      }
      
      return {
        publicar: false,
        accion: 'RECHAZAR_MENSAJE',
        mensaje: `Mensaje rechazado (Advertencia ${historial.infracciones}/3)`
      };
    }
    
    return {
      publicar: true,
      accion: null,
      mensaje: mensaje
    };
  }
}

// Ejemplo en un servidor WebSocket
const chat = new ChatModerado('tu_api_key_aqui');

socket.on('nuevo_mensaje', async (data) => {
  const resultado = await chat.procesarMensaje(data.userId, data.texto);
  
  if (resultado.publicar) {
    io.emit('mensaje', { user: data.userId, texto: data.texto });
  } else {
    socket.emit('error', { mensaje: resultado.mensaje });
  }
});
```

### 3. An√°lisis de Reportes de la Comunidad

```python
def generar_reporte_moderacion(comentarios):
    """
    Analiza una lista de comentarios y genera un reporte.
    
    Args:
        comentarios: Lista de dicts con {id, autor, texto}
    
    Returns:
        Reporte con estad√≠sticas y contenido problem√°tico
    """
    resultados = []
    
    for comentario in comentarios:
        moderacion = moderar_contenido(comentario['texto'])
        resultados.append({
            **comentario,
            'moderacion': moderacion
        })
    
    # Generar estad√≠sticas
    total = len(resultados)
    toxicos = sum(1 for r in resultados if r['moderacion']['is_toxic'])
    
    # Identificar los peores infractores
    por_autor = {}
    for r in resultados:
        autor = r['autor']
        if autor not in por_autor:
            por_autor[autor] = {'total': 0, 'toxicos': 0}
        por_autor[autor]['total'] += 1
        if r['moderacion']['is_toxic']:
            por_autor[autor]['toxicos'] += 1
    
    infractores = [
        {'autor': k, **v, 'ratio': v['toxicos']/v['total']}
        for k, v in por_autor.items()
        if v['toxicos'] >= 2
    ]
    infractores.sort(key=lambda x: x['ratio'], reverse=True)
    
    return {
        'resumen': {
            'total_comentarios': total,
            'comentarios_toxicos': toxicos,
            'porcentaje_toxicidad': f"{(toxicos/total)*100:.1f}%"
        },
        'infractores_frecuentes': infractores[:10],
        'comentarios_bloqueados': [
            r for r in resultados 
            if r['moderacion']['toxicity_score'] >= 0.7
        ]
    }

# Ejemplo
reporte = generar_reporte_moderacion(lista_comentarios)
print(f"üìä Toxicidad en la comunidad: {reporte['resumen']['porcentaje_toxicidad']}")
```

---

## ‚ö†Ô∏è C√≥digos de Error

| C√≥digo | Significado | Soluci√≥n |
|--------|-------------|----------|
| `400` | Texto inv√°lido (vac√≠o o muy largo) | Aseg√∫rate de enviar entre 1 y 5000 caracteres |
| `401` | Token inv√°lido o expirado | Obt√©n un nuevo token desde el dashboard |
| `402` | Sin cr√©ditos disponibles | Recarga tu saldo en apisdom.com |
| `429` | L√≠mite de peticiones excedido | Espera antes de reintentar |
| `500` | Error interno del servidor | Reintenta en unos segundos |

---

## ÔøΩ Transparencia T√©cnica

> **Pol√≠tica de Apisdom**: Creemos que los desarrolladores merecen saber exactamente c√≥mo funcionan las APIs que usan. Esta secci√≥n documenta los detalles t√©cnicos verificados directamente del c√≥digo fuente.

### C√≥mo Funciona Internamente

```
Tu texto ‚Üí Tokenizaci√≥n (toxic-bert) ‚Üí Inferencia ‚Üí C√°lculo ‚Üí Respuesta
         ‚Üì                            ‚Üì            ‚Üì
         512 tokens m√°x           CPU-bound    max_score > 0.7 = is_toxic
                                  (threadpool)
```

### Detalles Verificados del C√≥digo

| Aspecto | Valor Real | Archivo Fuente |
|---------|------------|----------------|
| **Modelo** | `unitary/toxic-bert` | moderation_service.py l√≠nea 28 |
| **Pipeline** | `text-classification` de HuggingFace | moderation_service.py |
| **Truncamiento** | 512 tokens (c√≥digo: `text[:512]`) | moderation_service.py l√≠nea 45 |
| **Threshold is_toxic** | `max_score > 0.7` | moderation_service.py l√≠nea 71 |
| **Ejecuci√≥n** | `run_in_threadpool` (no bloquea async) | moderation_service.py |

### C√°lculo de `is_toxic` y `toxicity_score`

```python
# C√≥digo real simplificado (moderation_service.py)
for result in results:
    label = result["label"].lower()
    score = float(result["score"])
    categories[label] = score
    max_score = max(max_score, score)

is_toxic = max_score > 0.7  # Threshold fijo
toxicity_score = round(max_score, 3)
```

**Implicaci√≥n**: `is_toxic` es una **simplificaci√≥n binaria** basada en el score m√°s alto. Si necesitas umbrales personalizados, usa `toxicity_score` directamente en tu l√≥gica.

### Cr√©ditos: Flujo Real

```
1. Verificar cr√©ditos ANTES de ejecutar modelo (CreditChecker.check_credits)
   ‚Üì Si no hay cr√©ditos ‚Üí HTTP 402 (modelo NO ejecutado, CPU no consumida)
   
2. Ejecutar inferencia toxic-bert
   
3. Consumir cr√©dito DESPU√âS de √©xito (CreditChecker.consume_credit)
   ‚Üì + Registrar uso en Redis para analytics
```

### Por Qu√© `categories` Puede Variar

El modelo `unitary/toxic-bert` retorna **solo las etiquetas que detecta con confianza**. No siempre devuelve todas las categor√≠as. Tu c√≥digo debe:

```python
# ‚ùå INCORRECTO - Puede fallar con KeyError
if resultado['categories']['threat'] >= 0.5:

# ‚úÖ CORRECTO - Usa .get() con default
if resultado['categories'].get('threat', 0) >= 0.5:
```

---

## ÔøΩüìù Notas Importantes

### Umbrales Recomendados por Tipo de Plataforma

| Tipo de Plataforma | Umbral Toxicidad | Umbral Amenazas | Umbral Identidad |
|-------------------|------------------|-----------------|------------------|
| Foro para ni√±os | 0.3 | 0.2 | 0.2 |
| Red social general | 0.5 | 0.4 | 0.4 |
| Foro de adultos | 0.7 | 0.4 | 0.5 |
| Chat privado | 0.8 | 0.5 | 0.6 |

### Sobre Falsos Positivos

El modelo puede marcar como t√≥xico:
- Discusiones sobre temas sensibles (pol√≠tica, religi√≥n)
- Citas de contenido t√≥xico para criticarlo
- Sarcasmo o humor negro

**Recomendaci√≥n**: Para contenido cerca del umbral (0.4-0.7), implementa revisi√≥n humana.

### Idiomas Soportados

El modelo `unitary/toxic-bert` fue entrenado principalmente en ingl√©s:
- ‚úÖ **Ingl√©s**: Rendimiento √≥ptimo
- ‚ö†Ô∏è **Espa√±ol y otros idiomas**: Funciona pero con precisi√≥n reducida

**Nota honesta:** Si tu contenido es principalmente en espa√±ol, el modelo puede tener m√°s falsos negativos (contenido t√≥xico no detectado) o falsos positivos.

---

## üîó Recursos Relacionados

- [Sentiment API](./SENTIMENT_API.md) - An√°lisis de sentimiento
- [Prediction API](./PREDICTION_API.md) - Predicciones de series temporales

---

## üí¨ ¬øNecesitas Ayuda?

üìß soporte@apisdom.com
